{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm\n",
    "from sklearn import kernel_ridge\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.utils import resample\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import svm\n",
    "from numpy import loadtxt\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for loading data and producing final CSV \n",
    "\n",
    "'''\n",
    "eliminate highly correlated features\n",
    "'''\n",
    "def to_be_eliminated(df):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    return to_drop\n",
    "\n",
    "'''\n",
    "loading training and test datasets\n",
    "'''\n",
    "def load_data():\n",
    "    X_train = pd.read_csv(\"X_train.csv\")\n",
    "    X_test = pd.read_csv(\"X_test.csv\")\n",
    "    y_train = pd.read_csv(\"y_train.csv\")\n",
    "     \n",
    "    #dropping id column\n",
    "    X_train = X_train.drop('id', axis = 1)\n",
    "    X_test = X_test.drop('id', axis = 1)\n",
    "    y_train = y_train.drop('id', axis = 1)\n",
    "   \n",
    "    #reshuffling data \n",
    "    X_train['y'] = y_train\n",
    "    X_train = X_train.sample(frac=1).reset_index(drop=True)\n",
    "    y_train = X_train['y']\n",
    "    X_train = X_train.drop('y', axis = 1)\n",
    "    \n",
    "    to_drop = to_be_eliminated(X_train)\n",
    "    \n",
    "    for i in range(len(to_drop)):\n",
    "        X_train = X_train.drop(to_drop[i], axis = 1)\n",
    "    \n",
    "    for i in range(len(to_drop)):\n",
    "        X_test = X_test.drop(to_drop[i], axis = 1)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "'''\n",
    "produce submission file\n",
    "'''\n",
    "def produce_solution(y):\n",
    "    with open('out.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', lineterminator=\"\\n\")\n",
    "        writer.writerow(['id', 'y'])\n",
    "        for i in range(y.shape[0]):\n",
    "            writer.writerow([float(i), y[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#X and y are training x and y data \\n#X_test_original corresponds to X_test.csv as given in the task \\n\\nX, X_test_original, y = load_data() \\ny = y.ravel()\\nscores = np.array([])\\n\\nkf = KFold(n_splits=5)\\nBMAC_scores = np.array([])\\n\\nfor train_index, test_index in kf.split(X):\\n    #define X_train and y_train as data in training folds (model is fitted here)\\n    #similarly, X_test, y_test as data in test fold (model is evaluated here)\\n    X_train, X_test = X[train_index], X[test_index]\\n    X_train = pd.DataFrame(X_train)\\n    X_test = pd.DataFrame(X_test)\\n\\n    y_train, y_test = y[train_index], y[test_index]\\n    y_train = pd.DataFrame(y_train)\\n    y_test = pd.DataFrame(y_test)\\n    \\n    y_train.columns = [\\'y\\']\\n    y_test.columns = [\\'y\\']\\n\\n    #oversampling to offset class imbalance\\n    X_concat = pd.concat([X_train, y_train], axis=1)\\n    \\n    # separate minority and majority classes\\n    class_0 = X_concat[X_concat.y==0]\\n    class_1 = X_concat[X_concat.y==1]\\n    class_2 = X_concat[X_concat.y==2]\\n\\n    #upsample minority -- classes 0 and 2\\n    class_0_upsampled = resample(class_0,\\n                          replace=True, # sample with replacement\\n                          n_samples=len(class_1), # match number in majority class\\n                          random_state=27) \\n    class_2_upsampled = resample(class_2,\\n                          replace=True, # sample with replacement\\n                          n_samples=len(class_1), # match number in majority class\\n                          random_state=32)\\n\\n    upsampled = pd.concat([class_1, class_0_upsampled, class_2_upsampled])\\n   \\n    y_train = upsampled.y\\n    X_train = upsampled.drop(\\'y\\', axis=1)\\n    \\n    #1. Zero Mean, Unit Variance\\n    print(\"Standardize data\")\\n    scaler = preprocessing.StandardScaler().fit(X_train)\\n    X_train = scaler.transform(X_train)\\n    X_test = scaler.transform(X_test)\\n    \\n#################################################################\\n#begin fitting model to training folds -- X_train \\n\\n    #2. Outlier detection\\n    print(\"Outlier Detection\")\\n    isf = IsolationForest(n_estimators=100, contamination=0.30)\\n    outliers = isf.fit_predict(X_train)\\n\\n    #DBScan = DBSCAN(eps = .5, metric=\\'euclidean\\', min_samples = 30, n_jobs = -1)    \\n    #outliers = DBScan.fit_predict(X_train)\\n\\n    unique, counts = np.unique(outliers, return_counts=True)\\n    count_dict = dict(zip(unique, counts))\\n    X_train = X_train[outliers == 1]\\n    y_train = y_train[outliers == 1]\\n    \\n    \\n    #3. Feature selection \\n    print(\"Feature Selection\")\\n    select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\\n    select.fit(X_train, y_train)\\n    X_train = select.transform(X_train)\\n    \\n    print(\"Fitting the model\")\\n    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.6, n_estimators=100, max_depth=10)\\n    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\\n    clf.fit(X_train, y_train)\\n    \\n#end model fitting on X_train\\n############################################################\\n        \\n    #prediction \\n    print(\"Predicting\")\\n    #selecting features based on training results\\n    X_test = select.transform(X_test)\\n    pred = clf.predict(X_test)\\n    \\n    #scoring\\n    score = balanced_accuracy_score(y_test, pred)\\n    print(score)\\n    scores = np.append(scores,score)\\n    \\n##########################################################    \\n\\ntruth = np.mean(scores)\\nstd = np.std(scores)\\nprint(\"mean expected error: \", truth, \"std: \", std)\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Approach 1: \n",
    "\n",
    "model assessment via 10 fold CV \n",
    "class imbalance is taken care of by oversamplingfrom classes 0 and 2\n",
    "\n",
    "Important Note\n",
    "Always split into test and train sets BEFORE trying oversampling techniques!\n",
    "Oversampling before splitting the data can allow the exact same observations \n",
    "to be present in both the test and train sets. This can allow our model to simply \n",
    "memorize specific data points and cause overfitting and poor generalization to the test data.\n",
    "'''\n",
    "'''\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "\n",
    "    #oversampling to offset class imbalance\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #upsample minority -- classes 0 and 2\n",
    "    class_0_upsampled = resample(class_0,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(class_1), # match number in majority class\n",
    "                          random_state=27) \n",
    "    class_2_upsampled = resample(class_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(class_1), # match number in majority class\n",
    "                          random_state=32)\n",
    "\n",
    "    upsampled = pd.concat([class_1, class_0_upsampled, class_2_upsampled])\n",
    "   \n",
    "    y_train = upsampled.y\n",
    "    X_train = upsampled.drop('y', axis=1)\n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#################################################################\n",
    "#begin fitting model to training folds -- X_train \n",
    "\n",
    "    #2. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=100, contamination=0.30)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "\n",
    "    #DBScan = DBSCAN(eps = .5, metric='euclidean', min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    \n",
    "    \n",
    "    #3. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = select.transform(X_train)\n",
    "    \n",
    "    print(\"Fitting the model\")\n",
    "    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.6, n_estimators=100, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "#end model fitting on X_train\n",
    "############################################################\n",
    "        \n",
    "    #prediction \n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test = select.transform(X_test)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    #scoring\n",
    "    score = balanced_accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "    scores = np.append(scores,score)\n",
    "    \n",
    "##########################################################    \n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data\n",
      "Feature Selection\n",
      "Fitting the model\n",
      "|   iter    |  target   | c0_weight | c1_weight | c2_weight |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8346  \u001b[0m | \u001b[0m 2.634   \u001b[0m | \u001b[0m 0.4472  \u001b[0m | \u001b[0m 2.602   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6256  \u001b[0m | \u001b[0m 1.776   \u001b[0m | \u001b[0m 4.354   \u001b[0m | \u001b[0m 1.654   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9062  \u001b[0m | \u001b[95m 7.349   \u001b[0m | \u001b[95m 2.442   \u001b[0m | \u001b[95m 4.894   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8347  \u001b[0m | \u001b[0m 6.127   \u001b[0m | \u001b[0m 2.592   \u001b[0m | \u001b[0m 2.374   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6345  \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6374  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9194  \u001b[0m | \u001b[95m 8.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 8.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8601  \u001b[0m | \u001b[0m 4.626   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 6.351   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.3333  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BEST APPROACH SO FAR\n",
    "    \n",
    "Approach 2: \n",
    "\n",
    "model assessment via 5 fold CV \n",
    "class imbalance is taken care of by undersampling from class 1 \n",
    "'''\n",
    "\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #2. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = pd.DataFrame(select.transform(X_train))\n",
    "   \n",
    "    '''\n",
    "    #3. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=300, contamination=0.38)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    #DBScan = DBSCAN(eps = .5, metric=”euclidean”,min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #4. Undersampling from class 1 to offset class imbalance\n",
    "    print('Undersampling')\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #undersample majority class (1)\n",
    "    class_1_under = resample(class_1,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=len(class_0), # match number in minority classes\n",
    "                          random_state=27) \n",
    "    undersampled = pd.concat([class_1_under, class_0, class_2])\n",
    "    y_train = undersampled.y\n",
    "    X_train = undersampled.drop('y', axis=1)\n",
    "    '''\n",
    "    #5. fitting model\n",
    "    print(\"Fitting the model\")\n",
    "    #clf = xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=300, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    class_weight = y_train.shape[0] / (3 * np.bincount((y_train.iloc[:,0]).astype(int)))\n",
    "    class_weights0 = { \n",
    "    0 : class_weight[0],\n",
    "    1 : class_weight[1],\n",
    "    2 : class_weight[2]\n",
    "    }\n",
    "    \n",
    "    ########## BO\n",
    "    def classifier(c0_weight=class_weight[0], c1_weight=class_weight[1], c2_weight=class_weight[2], \n",
    "                   xtrain=X_train, ytrain=y_train, xtest=X_test, ytest=y_test):\n",
    "        class_weights1 = { \n",
    "        0 : c0_weight,\n",
    "        1 : c1_weight,\n",
    "        2 : c2_weight\n",
    "        }\n",
    "        clf = svm.SVC(class_weight=class_weights1)\n",
    "\n",
    "        clf.fit(xtrain, ytrain)\n",
    "\n",
    "        #6. prediction \n",
    "        #print(\"Predicting\")\n",
    "        #selecting features based on training results\n",
    "        #_test_selected = pd.DataFrame(select.transform(xtest))  #note: transform was previosuly fitted on training folds\n",
    "        pred = clf.predict(xtrain)\n",
    "\n",
    "        #scoring\n",
    "        score = balanced_accuracy_score(ytrain, pred)\n",
    "        #print(score)\n",
    "        #scores = np.append(scores,score)\n",
    "        return score\n",
    "\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {\"c0_weight\": (0, 4), \"c1_weight\": (0, 3), \"c2_weight\": (0, 4)}\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=classifier,\n",
    "        pbounds=param_dist,\n",
    "        verbose=2,\n",
    "        random_state=5,\n",
    "    )\n",
    "\n",
    "    probe_params = {\"c0_weight\": class_weight[0], \"c1_weight\": class_weight[1], \"c2_weight\": class_weight[2]}\n",
    "    optimizer.probe(\n",
    "        params=probe_params,\n",
    "        lazy=True\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=3,\n",
    "        n_iter=80,\n",
    "    )\n",
    "\n",
    "    print(optimizer.max)\n",
    "    \n",
    "\n",
    "    ########## BO\n",
    "    \n",
    "    \n",
    "    class_weights_test = { \n",
    "    0 : optimizer.max['params']['c0_weight'],\n",
    "    1 : optimizer.max['params']['c1_weight'],\n",
    "    2 : optimizer.max['params']['c2_weight']\n",
    "    }\n",
    "    clf2 = svm.SVC(class_weight=class_weights_test)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test_selected = pd.DataFrame(select.transform(X_test))  #note: transform was previosuly fitted on training folds\n",
    "    pred2 = clf2.predict(X_test_selected)\n",
    "    #scoring\n",
    "    score2 = balanced_accuracy_score(y_test, pred2)\n",
    "    print('Test score:', score2)    \n",
    "    scores = np.append(scores,score2)\n",
    "    \n",
    "    \n",
    "##########################################################\n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "X_train, X_test, y_train = load_data() \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "class_weights0 = { \n",
    "0 : 2.67223382045929, \n",
    "1 : 0.44382801664355065, \n",
    "2 : 2.6834381551362685\n",
    "}\n",
    "\n",
    "class_weights1 = { \n",
    "0 : 2.6611226611226613, \n",
    "1 : 0.4435204435204435, \n",
    "2 : 2.7061310782241015\n",
    "}\n",
    "\n",
    "class_weights2 = { \n",
    "0 : 2.7176220806794054, \n",
    "1 : 0.44521739130434784, \n",
    "2 : 2.591093117408907\n",
    "}\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "    \n",
    "\n",
    "select = SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "select.fit(X_train, y_train)\n",
    "X_train = pd.DataFrame(select.transform(X_train))\n",
    "X_test = pd.DataFrame(select.transform(X_test))\n",
    "\n",
    "clf0 = svm.SVC(class_weight=class_weights0)\n",
    "clf1 = svm.SVC(class_weight=class_weights1)\n",
    "clf2 = svm.SVC(class_weight=class_weights2)\n",
    "eclf = VotingClassifier(estimators=[('clf0', clf0), ('clf1', clf1), ('clf2', clf2)], voting='hard')\n",
    "\n",
    "eclf.fit(X_train, y_train)\n",
    "pred = eclf.predict(pd.DataFrame(X_test))\n",
    "\n",
    "produce_solution(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Undersampling & Oversampling\n",
      "Fitting the model\n",
      "Predicting\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['0  ', '1  ', '2  ', '3  ', '4  ', '5  ', '6  ', '7  ', '8  ', '9  ', '10 ', '11 ', '12 ', '13 ', '14 ', '15 ', '16 ', '17 ', '18 ', '19 ', '20 ', '21 ', '22 ', '23 ', '24 ', '25 ', '26 ', '27 ', '28 ', '29 ', '30 ', '31 ', '32 ', '33 ', '34 ', '35 ', '36 ', '37 ', '38 ', '39 ', '40 ', '41 ', '42 ', '43 ', '44 ', '45 ', '46 ', '47 ', '48 ', '49 ', '50 ', '51 ', '52 ', '53 ', '54 ', '55 ', '56 ', '57 ', '58 ', '59 ', '60 ', '61 ', '62 ', '63 ', '64 ', '65 ', '66 ', '67 ', '68 ', '69 ', '70 ', '71 ', '72 ', '73 ', '74 ', '75 ', '76 ', '77 ', '78 ', '79 ', '80 ', '81 ', '82 ', '83 ', '84 ', '85 ', '86 ', '87 ', '88 ', '89 ', '90 ', '91 ', '92 ', '93 ', '94 ', '95 ', '96 ', '97 ', '98 ', '99 ', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205'] ['0  ', '1  ', '2  ', '3  ', '4  ', '5  ', '6  ', '7  ', '8  ', '9  ', '10 ', '11 ', '12 ', '13 ', '14 ', '15 ', '16 ', '17 ', '18 ', '19 ', '20 ', '21 ', '22 ', '23 ', '24 ', '25 ', '26 ', '27 ', '28 ', '29 ', '30 ', '31 ', '32 ', '33 ', '34 ', '35 ', '36 ', '37 ', '38 ', '39 ', '40 ', '41 ', '42 ', '43 ', '44 ', '45 ', '46 ', '47 ', '48 ', '49 ', '50 ', '51 ', '52 ', '53 ', '54 ', '55 ', '56 ', '57 ', '58 ', '59 ', '60 ', '61 ', '62 ', '63 ', '64 ', '65 ', '66 ', '67 ', '68 ', '69 ', '70 ', '71 ', '72 ', '73 ', '74 ', '75 ', '76 ', '77 ', '78 ', '79 ', '80 ', '81 ', '82 ', '83 ', '84 ', '85 ', '86 ', '87 ', '88 ', '89 ', '90 ', '91 ', '92 ', '93 ', '94 ', '95 ', '96 ', '97 ', '98 ', '99 ', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200']\nexpected 204, 203, 205, 201, 202 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-c5b5c7e34d35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;31m#selecting features based on training results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#note: transform was previosuly fitted on training folds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m#scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[0;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m                                                  validate_features=validate_features)\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# If output_margin is active, simply return the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1690\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['0  ', '1  ', '2  ', '3  ', '4  ', '5  ', '6  ', '7  ', '8  ', '9  ', '10 ', '11 ', '12 ', '13 ', '14 ', '15 ', '16 ', '17 ', '18 ', '19 ', '20 ', '21 ', '22 ', '23 ', '24 ', '25 ', '26 ', '27 ', '28 ', '29 ', '30 ', '31 ', '32 ', '33 ', '34 ', '35 ', '36 ', '37 ', '38 ', '39 ', '40 ', '41 ', '42 ', '43 ', '44 ', '45 ', '46 ', '47 ', '48 ', '49 ', '50 ', '51 ', '52 ', '53 ', '54 ', '55 ', '56 ', '57 ', '58 ', '59 ', '60 ', '61 ', '62 ', '63 ', '64 ', '65 ', '66 ', '67 ', '68 ', '69 ', '70 ', '71 ', '72 ', '73 ', '74 ', '75 ', '76 ', '77 ', '78 ', '79 ', '80 ', '81 ', '82 ', '83 ', '84 ', '85 ', '86 ', '87 ', '88 ', '89 ', '90 ', '91 ', '92 ', '93 ', '94 ', '95 ', '96 ', '97 ', '98 ', '99 ', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205'] ['0  ', '1  ', '2  ', '3  ', '4  ', '5  ', '6  ', '7  ', '8  ', '9  ', '10 ', '11 ', '12 ', '13 ', '14 ', '15 ', '16 ', '17 ', '18 ', '19 ', '20 ', '21 ', '22 ', '23 ', '24 ', '25 ', '26 ', '27 ', '28 ', '29 ', '30 ', '31 ', '32 ', '33 ', '34 ', '35 ', '36 ', '37 ', '38 ', '39 ', '40 ', '41 ', '42 ', '43 ', '44 ', '45 ', '46 ', '47 ', '48 ', '49 ', '50 ', '51 ', '52 ', '53 ', '54 ', '55 ', '56 ', '57 ', '58 ', '59 ', '60 ', '61 ', '62 ', '63 ', '64 ', '65 ', '66 ', '67 ', '68 ', '69 ', '70 ', '71 ', '72 ', '73 ', '74 ', '75 ', '76 ', '77 ', '78 ', '79 ', '80 ', '81 ', '82 ', '83 ', '84 ', '85 ', '86 ', '87 ', '88 ', '89 ', '90 ', '91 ', '92 ', '93 ', '94 ', '95 ', '96 ', '97 ', '98 ', '99 ', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200']\nexpected 204, 203, 205, 201, 202 in input data"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BAD\n",
    "BAD\n",
    "BAD\n",
    "\n",
    "Approach 3: \n",
    "\n",
    "model assessment via 5 fold CV \n",
    "class imbalance is taken care by performing the following 2 steps\n",
    "step 1: undersample from class 1 \n",
    "step 2: oversample from class 0 and 2\n",
    "this is a hybrid between approach 1 and 2 \n",
    "we choose the resampling in order to end up with balanced sampled \n",
    "each of which contains exactly half of the initial datapoints in class 1\n",
    "'''\n",
    "'''\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #2. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = pd.DataFrame(select.transform(X_train))\n",
    "    \n",
    "    #3. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=300, contamination=0.38)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    #DBScan = DBSCAN(eps = .5, metric=”euclidean”,min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "    \n",
    "    #4. Undersampling from class 1 to offset class imbalance\n",
    "    print('Undersampling & Oversampling')\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #undersample majority class (1)\n",
    "    class_1_under = resample(class_1,\n",
    "                          replace=False, # sample without replacement\n",
    "                          n_samples=min(1000,len(class_1)), \n",
    "                          random_state=27) \n",
    "    class_0_over = resample(class_0,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples= min(1000,len(class_1)), \n",
    "                          random_state=27)\n",
    "    class_2_over = resample(class_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=min(1000,len(class_1)), \n",
    "                          random_state=27)    \n",
    "    \n",
    "    undersampled = pd.concat([class_1_under, class_0_over, class_2_over])\n",
    "    y_train = undersampled.y\n",
    "    X_train = undersampled.drop('y', axis=1)\n",
    "      \n",
    "    #5. fitting model\n",
    "    print(\"Fitting the model\")\n",
    "    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=300, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    #6. prediction \n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test = pd.DataFrame(select.transform(X_test))  #note: transform was previosuly fitted on training folds\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    #scoring\n",
    "    score = balanced_accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "    scores = np.append(scores,score)\n",
    "\n",
    "##########################################################\n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data\n",
      "Feature Selection\n",
      "Outlier Detection\n",
      "Fitting the model\n",
      "Predicting\n",
      "0.5357672053022566\n",
      "Standardize data\n",
      "Feature Selection\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-284573868b3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Feature Selection\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mselect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\from_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \"Since 'prefit=True', call transform directly\")\n\u001b[0;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 330\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Approach 4: \n",
    "\n",
    "model assessment via 5 fold CV \n",
    "class imbalance is taken care of by undersampling from class 1 \n",
    "'''\n",
    "\n",
    "#X and y are training x and y data \n",
    "#X_test_original corresponds to X_test.csv as given in the task \n",
    "\n",
    "X, X_test_original, y = load_data() \n",
    "y = y.ravel()\n",
    "scores = np.array([])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "BMAC_scores = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #define X_train and y_train as data in training folds (model is fitted here)\n",
    "    #similarly, X_test, y_test as data in test fold (model is evaluated here)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    \n",
    "    y_train.columns = ['y']\n",
    "    y_test.columns = ['y']\n",
    "    \n",
    "    smote = SMOTE('minority')\n",
    "    X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "    '''\n",
    "    #undersampling from class 1 to offset class imbalance\n",
    "    X_concat = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # separate minority and majority classes\n",
    "    class_0 = X_concat[X_concat.y==0]\n",
    "    class_1 = X_concat[X_concat.y==1]\n",
    "    class_2 = X_concat[X_concat.y==2]\n",
    "\n",
    "    #upsample minority -- classes 0 and 2\n",
    "    class_1_under = resample(class_1,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=len(class_0), # match number in minority classes\n",
    "                          random_state=27) \n",
    "\n",
    "    undersampled = pd.concat([class_1_under, class_0, class_2])\n",
    "   \n",
    "    y_train = undersampled.y\n",
    "    X_train = undersampled.drop('y', axis=1)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #1. Zero Mean, Unit Variance\n",
    "    print(\"Standardize data\")\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#################################################################\n",
    "#begin fitting model to training folds -- X_train \n",
    "\n",
    "    #2. Feature selection \n",
    "    print(\"Feature Selection\")\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train = select.transform(X_train)\n",
    "\n",
    "    #3. Outlier detection\n",
    "    print(\"Outlier Detection\")\n",
    "    isf = IsolationForest(n_estimators=100, contamination=0.35)\n",
    "    outliers = isf.fit_predict(X_train)\n",
    "    \n",
    "    #DBScan = DBSCAN(eps = .5, metric=”euclidean”,min_samples = 30, n_jobs = -1)    \n",
    "    #outliers = DBScan.fit_predict(X_train)\n",
    "\n",
    "    unique, counts = np.unique(outliers, return_counts=True)\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    X_train = X_train[outliers == 1]\n",
    "    y_train = y_train[outliers == 1]\n",
    "    \n",
    "    print(\"Fitting the model\")\n",
    "    clf = xgb.XGBClassifier(random_state=42, learning_rate=0.5, n_estimators=100, max_depth=10)\n",
    "    #clf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "#end model fitting on X_train\n",
    "############################################################\n",
    "        \n",
    "    #prediction \n",
    "    print(\"Predicting\")\n",
    "    #selecting features based on training results\n",
    "    X_test = select.transform(X_test)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    #scoring\n",
    "    score = balanced_accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "    scores = np.append(scores,score)\n",
    "\n",
    "##########################################################\n",
    "\n",
    "truth = np.mean(scores)\n",
    "std = np.std(scores)\n",
    "print(\"mean expected error: \", truth, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
