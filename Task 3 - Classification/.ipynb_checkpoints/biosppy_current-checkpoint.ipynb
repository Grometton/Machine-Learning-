{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\n",
      "scipy: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import (PCA, LatentDirichletAllocation)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import random as rn\n",
    "from biosppy.signals import (ecg, tools)\n",
    "import pywt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import math\n",
    "from itertools import product\n",
    "# ============= CONSTS =============\n",
    "TRAIN_FILE_PATH = \"X_train.csv\"\n",
    "TARGET_FILE_PATH =  \"y_train.csv\"\n",
    "TEST_FILE_PATH = \"X_test.csv\"\n",
    "SAMPLE_FILE_PATH = \"sample.csv\"\n",
    "\n",
    "seed = 42\n",
    "NUM_MAX_COLS = 18154\n",
    "SAMPLING_RATE=300\n",
    "USE_WAVE_LETS = False\n",
    "my_cols = [\"id\"] + [\"x\" + str(i) for i in range(NUM_MAX_POINTS)]\n",
    "# ============= CONSTS =============\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loadind Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.read_csv(TRAIN_FILE_PATH, names=my_cols)[1:]\n",
    "xtrain.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "ytrain = pd.read_csv(TARGET_FILE_PATH)\n",
    "ytrain.drop(\"id\", axis=1, inplace = True)\n",
    "\n",
    "xtest =  pd.read_csv(TEST_FILE_PATH, names=my_cols)[1:]\n",
    "id_test = xtest.columns[0]\n",
    "xtest.drop(\"id\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions -- Submission and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sqrd_diff(rpeaks):\n",
    "    diff = np.diff(rpeaks)\n",
    "    mean_sqrd = np.mean(diff*diff)\n",
    "    \n",
    "    return mean_sqrd \n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    sample =  pd.read_csv(SAMPLE_FILE_PATH)\n",
    "    sample[\"y\"] = predictions\n",
    "    sample.to_csv(filename, index= False)\n",
    "\n",
    "def get_features_from_raw_qrs(signal, sampling_rate):\n",
    "    X = list()\n",
    "    ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(signal, sampling_rate, show = False)\n",
    "    \n",
    "    '''\n",
    "    Correct R-peak locations to the maximum within a tolerance\n",
    "    '''\n",
    "    rpeaks = ecg.correct_rpeaks(signal = signal, rpeaks=rpeaks, sampling_rate = sampling_rate, tol=0.01)  \n",
    "    \n",
    "    '''\n",
    "    extracting values of R-peaks -- Note: rpeaks gives only indices for R-peaks location\n",
    "    '''\n",
    "    peaks = signal[rpeaks]\n",
    "    \n",
    "    if len(heart_rate) < 2:\n",
    "        heart_rate = [0, 1]\n",
    "    if len(heart_rate_ts) < 2:\n",
    "        heart_rate_ts = [0, 1]\n",
    "           \n",
    "    X.append(np.mean(peaks))\n",
    "    X.append(np.min(peaks))\n",
    "    X.append(np.max(peaks))\n",
    "    X.append(np.std(peaks))\n",
    "    X.append(np.sqrt(mean_sqrd_diff(rpeaks)))        ## remove if results worsen\n",
    "    X.append(np.mean(np.diff(rpeaks)))\n",
    "    X.append(np.min(np.diff(rpeaks)))\n",
    "    X.append(np.max(np.diff(rpeaks)))\n",
    "    X.append(np.std(np.diff(rpeaks)))           \n",
    "    X.append(np.mean(heart_rate))\n",
    "    X.append(np.min(heart_rate))\n",
    "    X.append(np.max(heart_rate))\n",
    "    X.append(np.std(heart_rate))\n",
    "    X.append(np.mean(np.diff(heart_rate)))\n",
    "    X.append(np.min(np.diff(heart_rate)))\n",
    "    X.append(np.max(np.diff(heart_rate)))\n",
    "    X.append(np.std(np.diff(heart_rate)))\n",
    "    X.append(np.mean(np.diff(heart_rate_ts)))\n",
    "    X.append(np.min(np.diff(heart_rate_ts)))\n",
    "    X.append(np.min(np.diff(heart_rate_ts)))\n",
    "    X.append(np.max(np.diff(heart_rate_ts)))\n",
    "    X.append(np.std(np.diff(heart_rate_ts)))\n",
    "    X.append(np.sum(filtered-signal))\n",
    "    \n",
    "    X += list(np.mean(templates, axis=0))\n",
    "    X += list(np.abs(np.fft.rfft(np.mean(templates, axis=0), axis=0))[0:45])   ### adding FFT (choose only half of entries)\n",
    "    X += list(np.min(templates, axis=0))\n",
    "    X += list(np.max(templates, axis=0))\n",
    "    X = np.array(X)\n",
    "    \n",
    "    X[np.isnan(X)] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain features from raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c9cb9b045b48e8a56983f12ee942e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-b1f50d8a70fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msignal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_features_from_raw_qrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-259-ebcd676d6e41>\u001b[0m in \u001b[0;36mget_features_from_raw_qrs\u001b[1;34m(signal, sampling_rate)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_features_from_raw_qrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpeaks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplates_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheart_rate_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheart_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mecg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mecg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     '''\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\biosppy\\signals\\ecg.py\u001b[0m in \u001b[0;36mecg\u001b[1;34m(signal, sampling_rate, show)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# segment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mrpeaks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhamilton_segmenter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# correct R-peak locations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\biosppy\\signals\\ecg.py\u001b[0m in \u001b[0;36mhamilton_segmenter\u001b[1;34m(signal, sampling_rate)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;31m# Update Detection Threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[0mANP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoisepeakbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m         \u001b[0mAQRSP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqrspeakbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[0mDT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mANP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.475\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAQRSP\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mANP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3495\u001b[0m     \"\"\"\n\u001b[0;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[1;32m-> 3497\u001b[1;33m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[0;32m   3498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3499\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[1;34m(a, func, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3405\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3406\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3528\u001b[0m             \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3529\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3530\u001b[1;33m         \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = list()\n",
    "sampling_rate = float(SAMPLING_RATE)\n",
    "for id in tqdm(range(xtrain.shape[0])):\n",
    "    signal = np.array(pd.to_numeric(xtrain.iloc[id].dropna()))\n",
    "    features.append(get_features_from_raw_qrs(signal, sampling_rate))\n",
    "    \n",
    "    \n",
    "X = np.array(features)\n",
    "y = np.ravel(np.array(ytrain.values))\n",
    "\n",
    "features_test = list()\n",
    "for id in tqdm(range(xtest.shape[0])):\n",
    "    signal = np.array(pd.to_numeric(xtest.iloc[id].dropna()))\n",
    "    features_test.append(get_features_from_raw_qrs(signal, sampling_rate))\n",
    "    \n",
    "X_test = np.array(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2047,) (2047, 654)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "create subset of initial dataframe X for model selection \n",
    "'''\n",
    "X = pd.DataFrame(X) \n",
    "X['y'] = y\n",
    "X_sub = pd.DataFrame(X).sample(frac = 0.40, replace = False, axis=0)\n",
    "y_sub = X_sub['y']\n",
    "X_sub = X_sub.drop('y', axis = 1).values\n",
    "X = X.drop('y', axis = 1)\n",
    "print(y_sub.shape, X_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on models:\n",
    "\n",
    "Best results so far: \n",
    "\n",
    "0.7787005373717636 (2 fold GS CV) -- Model: Gradient_boosting_classifier\n",
    "0.825726141079\tpublic leaderboard\n",
    "{'classifier__learning_rate': 0.05, 'classifier__max_depth': 5, 'classifier__max_features': 40, 'classifier__n_estimators': 200}\n",
    "No fft included in features \n",
    "\n",
    "Next to run: \n",
    "same as above with fft included in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 480 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 960 out of 960 | elapsed: 85.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787005373717636\n",
      "{'classifier__learning_rate': 0.05, 'classifier__max_depth': 5, 'classifier__max_features': 40, 'classifier__n_estimators': 200}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## SVC APPROACH -- GRID-SEARCH CV\\n\\nsteps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", SVC())]\\npipeline = Pipeline(steps = steps)\\n\\nparameters = {\"classifier__kernel\": [\"rbf\", \"poly\"],\\n              \"classifier__gamma\": [\"auto\"],\\n              \"classifier__C\": [15,30,45,60,75],  \\n              \"classifier__class_weight\": [\"balanced\"],\\n              \"classifier__degree\": [2,4,6,8]\\n             }\\n\\ngrid = GridSearchCV(pipeline, parameters, cv = 5, scoring = scorer_f1, verbose = 2)\\n\\ngrid.fit(X, y)\\nprint(grid.best_score_)\\nprint(grid.best_params_)\\n\\nestimator = SVC(C = grid.best_params_[\\'classifier__C\\'], gamma = \\'auto\\', \\n                class_weight = \\'balanced\\', \\n                kernel = grid.best_params_[\\'classifier__kernel\\'], \\n                degree = grid.best_params_[\\'classifier__degree\\'])\\n\\nestimator.fit(xtrain_scaled, y)\\npred = estimator.predict(xtest_scaled)\\nmake_submission(\"prediction_trial.csv\", pred)\\n'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer_f1 = make_scorer(f1_score, greater_is_better=True, average='micro')\n",
    "\n",
    "'''\n",
    "Gradient Boosting APPROACH -- GRID-SEARCH CV\n",
    "Run with added FFT\n",
    "''' \n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", GradientBoostingClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__max_depth\": [5,10,15,20,25,40,50,100],\n",
    "              \"classifier__n_estimators\": [100,200,300],\n",
    "              \"classifier__learning_rate\": [3,1,0.3,0.05,0.01],\n",
    "              \"classifier__max_features\": [20,40,60,80]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 2, scoring = scorer_f1, verbose = 1)\n",
    "\n",
    "grid.fit(X_sub, y_sub)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "'''\n",
    "## SVC APPROACH -- GRID-SEARCH CV\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", SVC())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__kernel\": [\"rbf\", \"poly\"],\n",
    "              \"classifier__gamma\": [\"auto\"],\n",
    "              \"classifier__C\": [15,30,45,60,75],  \n",
    "              \"classifier__class_weight\": [\"balanced\"],\n",
    "              \"classifier__degree\": [2,4,6,8]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 5, scoring = scorer_f1, verbose = 2)\n",
    "\n",
    "grid.fit(X, y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "estimator = SVC(C = grid.best_params_['classifier__C'], gamma = 'auto', \n",
    "                class_weight = 'balanced', \n",
    "                kernel = grid.best_params_['classifier__kernel'], \n",
    "                degree = grid.best_params_['classifier__degree'])\n",
    "\n",
    "estimator.fit(xtrain_scaled, y)\n",
    "pred = estimator.predict(xtest_scaled)\n",
    "make_submission(\"prediction_trial.csv\", pred)\n",
    "'''\n",
    "\n",
    "'''\n",
    "## XGB APPROACH -- GRID-SEARCH CV\n",
    "\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), (\"classifier\", xgb.XGBClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__max_depth\": [5,10,15],\n",
    "              \"classifier__n_estimators\": [200],\n",
    "              \"classifier__learning_rate\": [0.05,0.1],\n",
    "              \"classifier__max_features\": [20,40]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv = 2, scoring = scorer_f1, verbose = 1)\n",
    "\n",
    "grid.fit(X_sub, y_sub)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "scaler.fit(X)\n",
    "x_train_scaled = scaler.transform(X)\n",
    "x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "'''\n",
    "change based on the output of the above\n",
    "'''\n",
    "estimator = GradientBoostingClassifier(n_estimators = 200, \n",
    "                                       max_depth = 5,\n",
    "                                       learning_rate = 0.05, \n",
    "                                       max_features= 40)\n",
    "\n",
    "\n",
    "'''\n",
    "estimator = GradientBoostingClassifier(n_estimators = grid.best_params_['classifier__n_estimators'], \n",
    "                                       max_depth = grid.best_params_['classifier__max_depth'],\n",
    "                                       learning_rate = grid.best_params_['classifier__learning_rate'], \n",
    "                                       max_features= grid.best_params_['classifier__max_features'])\n",
    "'''\n",
    "\n",
    "estimator.fit(xtrain_scaled, y)\n",
    "pred = estimator.predict(xtest_scaled)\n",
    "make_submission(\"prediction_trial.csv\", pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
