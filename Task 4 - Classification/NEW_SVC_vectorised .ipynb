{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pyeeg\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import log, floor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import yasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_eeg1 = pd.read_csv(\"train_eeg1.csv\").drop(\"Id\", axis = 1)\n",
    "xtrain_eeg2 = pd.read_csv(\"train_eeg2.csv\").drop(\"Id\", axis = 1)\n",
    "xtrain_emg = pd.read_csv(\"train_emg.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "ytrain = pd.read_csv(\"train_labels.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "xtest_eeg1 = pd.read_csv(\"test_eeg1.csv\").drop(\"Id\", axis = 1)\n",
    "xtest_eeg2 = pd.read_csv(\"test_eeg2.csv\").drop(\"Id\", axis = 1)\n",
    "xtest_emg = pd.read_csv(\"test_emg.csv\").drop(\"Id\", axis = 1)\n",
    "\n",
    "xtrain_eeg1 = xtrain_eeg1.values\n",
    "xtrain_eeg2 = xtrain_eeg2.values\n",
    "xtrain_emg = xtrain_emg.values\n",
    "\n",
    "xtest_eeg1 = xtest_eeg1.values \n",
    "xtest_eeg2 = xtest_eeg2.values\n",
    "xtest_emg = xtest_emg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if (ypred_crf[i] != ypred_boost[i] and  \\nypred_crf[i] != ypred_svm[i] and\\nypred_svm[i] != ypred_boost[i]): \\n\\nrand = np.random.choice(np.arange(1, 4), p=[p_crf,p_boost,p_svm])\\n\\npredictions[i] = rand'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _log_n(min_n, max_n, factor):\n",
    "    \"\"\"\n",
    "    Creates a list of integer values by successively multiplying a minimum\n",
    "    \"\"\"\n",
    "    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n",
    "    ns = [min_n]\n",
    "    for i in range(max_i + 1):\n",
    "        n = int(floor(min_n * (factor ** i)))\n",
    "        if n > ns[-1]:\n",
    "            ns.append(n)\n",
    "    return np.array(ns, dtype=np.int64)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def _linear_regression(x, y):\n",
    "    \"\"\"Fast linear regression using Numba.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : ndarray, shape (n_times,)\n",
    "        Variables\n",
    "    Returns\n",
    "    -------\n",
    "    slope : float\n",
    "        Slope of 1D least-square regression.\n",
    "    intercept : float\n",
    "        Intercept\n",
    "    \"\"\"\n",
    "    n_times = x.size\n",
    "    sx2 = 0\n",
    "    sx = 0\n",
    "    sy = 0\n",
    "    sxy = 0\n",
    "    for j in range(n_times):\n",
    "        sx2 += x[j] ** 2\n",
    "        sx += x[j]\n",
    "        sxy += x[j] * y[j]\n",
    "        sy += y[j]\n",
    "    den = n_times * sx2 - (sx ** 2)\n",
    "    num = n_times * sxy - sx * sy\n",
    "    slope = num / den\n",
    "    intercept = np.mean(y) - slope * np.mean(x)\n",
    "    return slope, intercept\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "\n",
    "def _dfa(x):\n",
    "    \"\"\"\n",
    "    Utility function for detrended fluctuation analysis\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    nvals = _log_n(4, 0.1 * N, 1.2)\n",
    "    walk = np.cumsum(x - x.mean())\n",
    "    fluctuations = np.zeros(len(nvals))\n",
    "\n",
    "    for i_n, n in enumerate(nvals):\n",
    "        d = np.reshape(walk[:N - (N % n)], (N // n, n))\n",
    "        ran_n = np.array([float(na) for na in range(n)])\n",
    "        d_len = len(d)\n",
    "        slope = np.empty(d_len)\n",
    "        intercept = np.empty(d_len)\n",
    "        trend = np.empty((d_len, ran_n.size))\n",
    "        for i in range(d_len):\n",
    "            slope[i], intercept[i] = _linear_regression(ran_n, d[i])\n",
    "            y = np.zeros_like(ran_n)\n",
    "            # Equivalent to np.polyval function\n",
    "            for p in [slope[i], intercept[i]]:\n",
    "                y = y * ran_n + p\n",
    "            trend[i, :] = y\n",
    "        # calculate standard deviation (fluctuation) of walks in d around trend\n",
    "        flucs = np.sqrt(np.sum((d - trend) ** 2, axis=1) / n)\n",
    "        # calculate mean fluctuation over all subsequences\n",
    "        fluctuations[i_n] = flucs.sum() / flucs.size\n",
    "\n",
    "    # Filter zero\n",
    "    nonzero = np.nonzero(fluctuations)[0]\n",
    "    fluctuations = fluctuations[nonzero]\n",
    "    nvals = nvals[nonzero]\n",
    "    if len(fluctuations) == 0:\n",
    "        # all fluctuations are zero => we cannot fit a line\n",
    "        dfa = np.nan\n",
    "    else:\n",
    "        dfa, _ = _linear_regression(np.log(nvals), np.log(fluctuations))\n",
    "    return dfa\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "def matrix_dfa(x):\n",
    "    dfa_ = np.zeros((x.shape[0],), dtype=np.float64)\n",
    "    for i in (np.arange(x.shape[0])):\n",
    "        dfa_[i] = _dfa(np.asarray(x[i], dtype=np.float64))\n",
    "    return np.reshape(dfa_, (-1, 1))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def _embed(x, order=3, delay=1):\n",
    "    \"\"\" VECTORIZED!! Kind of TESTED. Time-delay embedding. x is an (nxd) matrix\n",
    "    \"\"\"\n",
    "    N = x.shape[1]\n",
    "    if order * delay > N:\n",
    "        raise ValueError(\"Error: order * delay should be lower than x.size\")\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"Delay has to be at least 1.\")\n",
    "    if order < 2:\n",
    "        raise ValueError(\"Order has to be at least 2.\")\n",
    "    Y = np.zeros((x.shape[0], N - (order - 1) * delay, order))\n",
    "    for i in range(order):\n",
    "        Y[:, :, i] = x[:, i * delay:i * delay + Y.shape[1]]\n",
    "    return Y\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def hfd(X, Kmax):\n",
    "    \"\"\" VECTORIZED!!! TESTED: Matches the for loop output. Can test easily comparing\n",
    "    to the pyeeg.hfd() function. X now a (nxd) matrix.\n",
    "    Compute Higuchi Fractal Dimension of a time series X. kmax\n",
    "     is an HFD parameter\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    x = []\n",
    "    N = X.shape[1]\n",
    "    for k in (range(1, Kmax)):\n",
    "        # Lk = np.empty(shape=[0, ])\n",
    "        Lk = np.empty(shape=[X.shape[0], 1])\n",
    "        for m in range(0, k):\n",
    "            Lmk = 0\n",
    "            for i in range(1, int(np.floor((N - m) / k))):\n",
    "                Lmk += np.abs(X[:, m + i * k] - X[:, m + i * k - k])\n",
    "            Lmk = Lmk * (N - 1) / np.floor((N - m) / float(k)) / k\n",
    "            Lmk = np.reshape(Lmk, (Lmk.shape[0], 1))\n",
    "            Lk = np.hstack((Lk, Lmk))\n",
    "\n",
    "        # Remove that first placeholder column of zeros in Lk:\n",
    "        Lk = Lk[:, 1:]\n",
    "        L.append(np.log(np.mean(Lk, axis=1)))\n",
    "        x.append([np.log(float(1) / k), 1]) # Fix this!!!\n",
    "\n",
    "    (p, _, _, _) = np.linalg.lstsq(x, L)\n",
    "    return p[0]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def pfd(X):\n",
    "    \"\"\"VECTORIZED!!! TESTED, matches the 1d time series output. Now accepts (nxd) matrices as input\n",
    "    Compute Petrosian Fractal Dimension of a time series from either two\n",
    "    cases below:\n",
    "        1. X, the time series of type list (default)\n",
    "        2. D, the first order differential sequence of X (if D is provided,\n",
    "           recommended to speed up)\n",
    "    In case 1, D is computed using Numpy's difference function.\n",
    "    To speed up, it is recommended to compute D before calling this function\n",
    "    because D may also be used by other functions whereas computing it here\n",
    "    again will slow down.\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    diff = np.diff(X, axis=1)\n",
    "    N_delta = np.sum(diff[:, 1:-1] * diff[:, 0:-2] < 0, axis=1)\n",
    "    return np.log10(n) / (np.log10(n) + np.log10(n / (n + 0.4 * N_delta)))\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def fisher_info(X, Tau=3, DE=1, W=None):\n",
    "    \"\"\" VECTORIZED, TESTED, gives approximate results but not exact. Compute SVD Entropy from either two cases below:\n",
    "    \"\"\"\n",
    "    if W is None:\n",
    "        Y = _embed(X, Tau, DE)\n",
    "        W = np.linalg.svd(Y, compute_uv=0)\n",
    "        W = np.divide(W, np.reshape(np.sum(W, axis=1), (-1, 1)))  # normalize singular values\n",
    "    return -1 * np.sum(W * np.log(W), axis=1)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def extract_bandpower_eeg(signal, frequency = 128):\n",
    "    for i in (np.arange(signal.shape[0] / 100) + 1):\n",
    "        if i == 1:\n",
    "            df = yasa.bandpower(signal[0:int(100*i),:], sf=frequency)\n",
    "        else:\n",
    "            df = df.append(yasa.bandpower(signal[int(100*(i-1)):int(100*i),:], sf=frequency))\n",
    "    \n",
    "    df = df.set_index(np.arange(signal.shape[0]))\n",
    "    df = df.drop(columns = [\"FreqRes\",\"Relative\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "'''input must be np.ndarray'''\n",
    "\n",
    "def vectorized_adv_stat(signal, fs=128):\n",
    "    K_boundary = 10         # to be tuned\n",
    "    t_fisher = 12          # to be tuned\n",
    "    d_fisher = 40          # to be tuned\n",
    "    features_num = 11\n",
    "    threshold =  0.0009\n",
    "    advanced_stats = np.zeros((signal.shape[0],features_num))\n",
    "    # Missing fisher info and dfa\n",
    "    feat_array = np.array([\n",
    "                           fisher_info(signal, t_fisher, d_fisher),\n",
    "                           pfd(signal),\n",
    "                           hfd(signal, K_boundary),\n",
    "                           np.sum((np.power(np.abs(signal),(-0.3)) > 20), axis=1),\n",
    "                           np.sum((np.abs(signal)) > threshold, axis=1),\n",
    "                           np.std(np.power(np.abs(signal),(0.05)), axis=1),\n",
    "                           np.sqrt(np.mean(np.power(np.diff(signal, axis=1), 2), axis=1)),\n",
    "                           np.mean(np.abs(np.diff(signal, axis=1)), axis=1),\n",
    "                           np.mean(np.power(signal, 5), axis=1),\n",
    "                           np.sum(np.power(signal, 2), axis=1)\n",
    "                           ]).T\n",
    "    #feat_array = np.concatenate((feat_array, matrix_dfa(signal)), axis=1) # Concatenate the lengthy dfa calculation\n",
    "\n",
    "    return feat_array\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def simple_stats(signal, fs = 128):\n",
    "           \n",
    "    if (len(signal.shape) > 1) and (signal.shape[1]!=1):\n",
    "        simple_stats = np.array([np.mean(signal, axis=1), \n",
    "                        np.median(signal, axis=1),\n",
    "                        np.std(signal, axis=1), \n",
    "                        np.max(signal, axis=1),\n",
    "                        np.min(signal, axis=1), \n",
    "                        kurtosis(signal, axis=1),\n",
    "                        skew(signal, axis=1), \n",
    "                        np.sum(np.abs(signal), axis = 1)]).T\n",
    "                        \n",
    "    else:\n",
    "        print(\"Not Tested with this input!\")\n",
    "        simple_stats =  np.array([np.mean(signal), \n",
    "                        np.median(signal), \n",
    "                        np.std(sigal),\n",
    "                        np.max(signal), \n",
    "                        np.min(signal), \n",
    "                        float(kurtosis(signal)),\n",
    "                        float(skew(signal))])\n",
    "        \n",
    "    \n",
    "\n",
    "    return (simple_stats)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def EEG_feature_extraction(signal, fs = 128): \n",
    "    eeg_features = np.concatenate((extract_bandpower_eeg(signal, frequency = 128), vectorized_adv_stat(signal, fs=128),\n",
    "                                   simple_stats(signal, fs = 128)), axis = 1)\n",
    "    \n",
    "    return(eeg_features)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "def EMG_feature_extraction(signal, fs = 128):\n",
    "    \n",
    "    features_num_emg = 6\n",
    "    \n",
    "    if (len(signal.shape) > 1) and (signal.shape[1]!=1):\n",
    "        simple_stats = np.array([np.mean(signal, axis=1), \n",
    "                        np.median(signal, axis=1),\n",
    "                        np.std(signal, axis=1), \n",
    "                        np.max(signal, axis=1),\n",
    "                        np.min(signal, axis=1), \n",
    "                        kurtosis(signal, axis=1),\n",
    "                        skew(signal, axis=1), \n",
    "                        pd.Series(np.sum(np.abs(signal), axis = 1))]).T\n",
    "                        \n",
    "    else:\n",
    "        print(\"Not Tested with this input!\")\n",
    "        simple_stats =  np.array([np.mean(signal), \n",
    "                        np.median(signal), \n",
    "                        np.std(sigal),\n",
    "                        np.max(signal), \n",
    "                        np.min(signal), \n",
    "                        float(kurtosis(signal)),\n",
    "                        float(skew(signal))])\n",
    "\n",
    "    advanced_stats = np.zeros((signal.shape[0],features_num_emg))\n",
    "    for i in tqdm((np.arange(signal.shape[0]))):\n",
    "        feat_array = np.array([\n",
    "                              np.median(signal[i,:] ** 2), \n",
    "                              np.median(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.std(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.sum(np.abs(np.diff(signal[i,:]))), \n",
    "                              np.mean(np.power(np.diff(signal[i,:]), 2)),\n",
    "                              np.std(abs(signal[i,:]))\n",
    "                            ])\n",
    "\n",
    "        advanced_stats[i, :] = feat_array \n",
    "        \n",
    "        union_smpl_adv = np.concatenate((simple_stats, advanced_stats), axis = 1)\n",
    "\n",
    "    return(union_smpl_adv)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "'''change for submissions predictions'''\n",
    "\n",
    "def hard_voter(ypred_crf, ypred_svm, \n",
    "               ypred_logi, ypred_KNN,\n",
    "               ypred_RNN, ypred_SGD):\n",
    "    \n",
    "    predictions = np.zeros(xtest.shape[0])   \n",
    "    \n",
    "    for i in range(xtest.shape[0]):\n",
    "\n",
    "        counts = np.bincount(np.array([ypred_crf[i],ypred_svm[i], \n",
    "                    ypred_logi[i], ypred_KNN[i], ypred_RNN[i], ypred_SGD[i]]))\n",
    "        predictions[i] = np.argmax(counts)\n",
    "        \n",
    "    return(np.asarray(predictions, dtype=np.int32)) \n",
    "\n",
    "\n",
    "'''if (ypred_crf[i] != ypred_boost[i] and  \n",
    "ypred_crf[i] != ypred_svm[i] and\n",
    "ypred_svm[i] != ypred_boost[i]): \n",
    "\n",
    "rand = np.random.choice(np.arange(1, 4), p=[p_crf,p_boost,p_svm])\n",
    "\n",
    "predictions[i] = rand'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features --- Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7218ad9bc6f4500a55b2c896687160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22389af507a14fb0b7a7b00da1e8192b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(64800, 60) (43200, 60)\n"
     ]
    }
   ],
   "source": [
    "xtrain_eeg1_processed = EEG_feature_extraction(xtrain_eeg1, 128)\n",
    "xtrain_eeg2_processed = EEG_feature_extraction(xtrain_eeg2, 128)\n",
    "xtrain_emg_processed = EMG_feature_extraction(xtrain_emg, 128)\n",
    "xtrain = np.concatenate((xtrain_eeg1_processed, \n",
    "                         xtrain_eeg2_processed, \n",
    "                         xtrain_emg_processed), \n",
    "                         axis = 1)\n",
    "\n",
    "xtest_eeg1_processed = EEG_feature_extraction(xtest_eeg1, 128)\n",
    "xtest_eeg2_processed = EEG_feature_extraction(xtest_eeg2, 128)\n",
    "xtest_emg_processed = EMG_feature_extraction(xtest_emg, 128)\n",
    "xtest = np.concatenate((xtest_eeg1_processed, \n",
    "                         xtest_eeg2_processed, \n",
    "                         xtest_emg_processed), \n",
    "                         axis = 1)\n",
    "\n",
    "print(xtrain.shape, xtest.shape)\n",
    "\n",
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "xtrain_rescaled = scaler.fit_transform(xtrain_grid)\n",
    "xtest_rescaled = scaler.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate mutual information of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = mutual_info_classif(xtrain, ytrain, discrete_features='auto', n_neighbors = 10, copy=True, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave_out_feat = np.where(mutual_info > 0.08)\n",
    "#leave_out_feat = list(leave_out_feat)[0]\n",
    "\n",
    "'''update xtrain and xtest'''\n",
    "\n",
    "xtrain_filter = xtrain#[:,leave_out_feat]\n",
    "xtest_filter = xtest#[:,leave_out_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS SVM  --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  36.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  48.5s\n",
      "[CV] classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.01, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  47.1s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  18.0s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  29.6s\n",
      "[CV] classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.1, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  27.4s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  15.7s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  25.3s\n",
      "[CV] classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf \n",
      "[CV]  classifier__C=0.3, classifier__class_weight=balanced, classifier__gamma=auto, classifier__kernel=rbf, total=  25.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919127323819208\n",
      "{'classifier__C': 0.01, 'classifier__class_weight': 'balanced', 'classifier__gamma': 'auto', 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", SVC())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__kernel\": [\"rbf\"],\n",
    "              \"classifier__gamma\": [\"auto\"],\n",
    "              \"classifier__C\": [0.01, 0.1,0.3],  \n",
    "              \"classifier__class_weight\": [\"balanced\"]\n",
    "             }\n",
    "grid_svm = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_svm.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Gradient Boosting Method (very slow search)  --- MANUAL OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient Boosting APPROACH -- GRID-SEARCH CV\\n'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Gradient Boosting APPROACH -- GRID-SEARCH CV\n",
    "''' \n",
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", GradientBoostingClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__max_depth\": [20],\n",
    "              \"classifier__n_estimators\": [100],\n",
    "              \"classifier__learning_rate\": [0.2],\n",
    "              \"classifier__max_features\": [20]\n",
    "             }\n",
    "\n",
    "grid_boost = GridSearchCV(pipeline, parameters, cv = 3,scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid_boost.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid_boost.best_score_)\n",
    "print(grid_boost.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Logistic  --- OPT DONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   24.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9267708371037586\n",
      "{'classifier__C': 0.01, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 10, 'classifier__multi_class': 'auto', 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'classifier__verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", LogisticRegression(fit_intercept=True))]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__multi_class\": [\"auto\"],\n",
    "              \"classifier__C\": [0.02,0.01],  \n",
    "              \"classifier__class_weight\": [\"balanced\"], \n",
    "              \"classifier__max_iter\": [10,15,20],\n",
    "              \"classifier__verbose\": [0],\n",
    "              \"classifier__penalty\": ['l2'],\n",
    "              \"classifier__solver\": ['liblinear']\n",
    "             }\n",
    "grid_logi = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_logi.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_logi.best_score_)\n",
    "print(grid_logi.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS Random Forest  --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8849094425926666\n",
      "{'classifier__class_weight': 'balanced', 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 100, 'classifier__min_samples_split': 100, 'classifier__n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "steps = [(\"scaler\", StandardScaler()), (\"classifier\", RandomForestClassifier(criterion='entropy', max_features='auto'))]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__n_estimators\": [20],\n",
    "              \"classifier__max_depth\": [5], \n",
    "              \"classifier__class_weight\": [\"balanced\"], \n",
    "              \"classifier__min_samples_split\":[100,200], \n",
    "              \"classifier__min_samples_leaf\": [100]\n",
    "             }\n",
    "grid_RandFor = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "\n",
    "grid_RandFor.fit(xtrain_grid, ytrain.values.ravel())\n",
    "print(grid_RandFor.best_score_)\n",
    "print(grid_RandFor.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS KNN (Very slow search)  --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 15.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9065325243946395\n",
      "{'classifier__leaf_size': 200, 'classifier__n_neighbors': 20, 'classifier__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", KNeighborsClassifier())]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__n_neighbors\": [5,10,20],\n",
    "              \"classifier__weights\": ['uniform'],\n",
    "              \"classifier__leaf_size\": [200]\n",
    "             }\n",
    "\n",
    "grid_KNN = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid_KNN.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid_KNN.best_score_)\n",
    "print(grid_KNN.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GS SGD Classifier --- OPT DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8601686473634104\n",
      "{'classifier__alpha': 0.0001, 'classifier__learning_rate': 'optimal', 'classifier__loss': 'hinge', 'classifier__max_iter': 100, 'classifier__penalty': 'l2', 'classifier__power_t': 2}\n"
     ]
    }
   ],
   "source": [
    "xtrain_grid = pd.DataFrame(xtrain)\n",
    "\n",
    "steps = [(\"scaler\", preprocessing.StandardScaler()), \n",
    "         (\"classifier\", SGDClassifier(shuffle = True, fit_intercept=True))]\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "parameters = {\"classifier__penalty\": ['l2'],\n",
    "              \"classifier__alpha\": [0.0001],\n",
    "              \"classifier__max_iter\": [100],\n",
    "              \"classifier__loss\": ['hinge'],\n",
    "              \"classifier__learning_rate\": ['optimal'],\n",
    "              \"classifier__power_t\":[2]\n",
    "              }\n",
    "\n",
    "grid_SGD = GridSearchCV(pipeline, parameters, cv = 3, scoring = 'balanced_accuracy', verbose = 1)\n",
    "grid_SGD.fit(xtrain_grid, ytrain.values.ravel())\n",
    "\n",
    "print(grid_SGD.best_score_)\n",
    "print(grid_SGD.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Validation --- train on subj 1,2 --- test on 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b7d396caf4a01bee1dc9d29cba2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479a4eb6f3664deea7321427b1486576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''extracting features'''\n",
    "\n",
    "subj12_eeg1 = xtrain_eeg1[0:43200,:]\n",
    "subj12_eeg2 = xtrain_eeg2[0:43200,:]\n",
    "subj12_emg = xtrain_emg[0:43200,:]\n",
    "ytrain_subj12 = np.reshape(ytrain.values, (-1,))[0:43200]\n",
    "\n",
    "xtrain_subj12_eeg_1 = EEG_feature_extraction(subj12_eeg1, 128)  \n",
    "xtrain_subj12_eeg_2 = EEG_feature_extraction(subj12_eeg2, 128)\n",
    "xtrain_subj12_emg = EMG_feature_extraction(subj12_emg, 128)\n",
    "xtrain_subj12 = np.concatenate((xtrain_subj12_eeg_1, xtrain_subj12_eeg_2, xtrain_subj12_emg), axis = 1)\n",
    "\n",
    "\n",
    "'''creating validation data'''\n",
    "\n",
    "subj3_eeg1 = xtrain_eeg1[43200:64800,:]\n",
    "subj3_eeg2 = xtrain_eeg2[43200:64800,:]\n",
    "subj3_emg = xtrain_emg[43200:64800,:]\n",
    "ytest_subj3 = np.reshape(ytrain.values, (-1,))[43200:64800]\n",
    "\n",
    "xtest_subj3_eeg_1 = EEG_feature_extraction(subj3_eeg1, 128)\n",
    "xtest_subj3_eeg_2 = EEG_feature_extraction(subj3_eeg2, 128)\n",
    "xtest_subj3_emg = EMG_feature_extraction(subj3_emg, 128)\n",
    "xtest_subj3 = np.concatenate((xtest_subj3_eeg_1, xtest_subj3_eeg_2, xtest_subj3_emg), axis = 1)\n",
    "\n",
    "xtrain_subj12_rescaled = preprocessing.StandardScaler().fit_transform(xtrain_subj12)\n",
    "xtest_subj3_rescaled = preprocessing.StandardScaler().fit_transform(xtest_subj3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM done\n",
      "Multinomial Logistic done\n",
      "SGD done\n",
      "Gradient Boosting done\n",
      "Random Forest Done\n",
      "KNN done\n",
      "SGD done\n",
      "CRF Done\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-293-5ec89fea6690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhard_voter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mypred_crf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_boost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_svm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_logi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_RandFor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_KNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_SGD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[0mBMAC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbalanced_accuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest_subj3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-292-f308dacef6cc>\u001b[0m in \u001b[0;36mhard_voter\u001b[1;34m(ypred_crf, ypred_boost, ypred_svm, ypred_logi, ypred_RandFor, ypred_KNN, ypred_SGD)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest_subj3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         counts = np.bincount(np.array([ypred_crf[i], ypred_boost[i],ypred_svm[i], \n\u001b[0m\u001b[0;32m    305\u001b[0m                     ypred_logi[i], ypred_RandFor[i], ypred_KNN[i], ypred_SGD[i]]))\n\u001b[0;32m    306\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "### '''Testing the models'''\n",
    "### \n",
    "### '''SVM'''\n",
    "### \n",
    "### clf_svm = svm.SVC(kernel='rbf', C = 0.01, gamma = 'auto', class_weight = 'balanced')\n",
    "### clf_svm.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_svm = clf_svm.predict(xtest_subj3)\n",
    "### print('SVM done')\n",
    "### \n",
    "### \n",
    "### '''Multinomial Logistic Regression'''\n",
    "### \n",
    "### clf_logi = LogisticRegression(random_state=10, C = 0.01, class_weight = 'balanced', max_iter = 10, \n",
    "###                               multi_class='auto', penalty = 'l2', solver = 'liblinear')\n",
    "### clf_logi.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_logi = clf_logi.predict(xtest_subj3)\n",
    "### print('Multinomial Logistic done')\n",
    "### \n",
    "### \n",
    "### \n",
    "### '''SGD'''\n",
    "### \n",
    "### clf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = 'optimal', loss = 'hinge', max_iter = 1000, \n",
    "###                         penalty = 'l1', power_t = 2, shuffle = True, fit_intercept=True)\n",
    "### clf_SGD.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_SGD = clf_SGD.predict(xtest_subj3)\n",
    "### print('SGD done')\n",
    "### \n",
    "### \n",
    "### \n",
    "### '''Boosting Classifier'''\n",
    "### \n",
    "### clf_boost = GradientBoostingClassifier(n_estimators = 200, max_depth = 8, \n",
    "###                                        learning_rate = 0.1, max_features = 20)\n",
    "### clf_boost.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_boost = clf_boost.predict(xtest_subj3)\n",
    "### print('Gradient Boosting done')\n",
    "### \n",
    "### \n",
    "### \n",
    "### '''Random Forest'''\n",
    "### \n",
    "### clf_RandFor = RandomForestClassifier(max_depth=5, random_state=1, class_weight = 'balanced', \n",
    "###                          min_samples_leaf = 100, min_samples_split = 100, n_estimators = 20)\n",
    "### clf_RandFor.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_RandFor = clf_RandFor.predict(xtest_subj3)\n",
    "### print('Random Forest Done')\n",
    "### \n",
    "### \n",
    "### '''KNN'''\n",
    "### \n",
    "### clf_KNN = KNeighborsClassifier(leaf_size = 200, n_neighbors = 20, weights = 'uniform')\n",
    "### clf_KNN.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_KNN = clf_KNN.predict(xtest_subj3)\n",
    "### print('KNN done')\n",
    "### \n",
    "### \n",
    "### '''SGD'''\n",
    "### \n",
    "### clf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = 'optimal', loss = 'hinge',\n",
    "###                        max_iter = 100, penalty = 'l2', power_t = 2, shuffle = True)\n",
    "### clf_SGD.fit(xtrain_subj12, ytrain_subj12)\n",
    "### ypred_SGD = clf_SGD.predict(xtest_subj3)\n",
    "### print('SGD done')\n",
    "### \n",
    "### \n",
    "### '''CRF'''\n",
    "### \n",
    "### ypred_crf = pd.read_csv(\"CRFc0.1w0m2000_subj3pred.csv\").drop(\"Id\", axis = 1)\n",
    "### ypred_crf = np.reshape(ypred_crf.values, (-1,))\n",
    "### print('CRF Done')\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8216955827937881\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "\n",
    "y_val = hard_voter(ypred_crf, ypred_boost, ypred_svm, ypred_logi, ypred_RandFor, ypred_KNN, ypred_SGD) \n",
    "BMAC = balanced_accuracy_score(ytest_subj3, y_val)\n",
    "\n",
    "print(BMAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load DFA data'''\n",
    "\n",
    "dfa_xtrain_eeg1 = pd.read_csv('dfa_xtrain_eeg1.csv', header = None)\n",
    "#dfa_xtrain_eeg1 = np.reshape(dfa_xtrain_eeg1.values, (-1,))\n",
    "\n",
    "dfa_xtrain_eeg2 = pd.read_csv('dfa_xtrain_eeg2.csv', header = None)\n",
    "#dfa_xtrain_eeg2 = np.reshape(dfa_xtrain_eeg2.values, (-1,))\n",
    "\n",
    "dfa_xtrain_emg = pd.read_csv('dfa_xtrain_emg.csv', header = None)\n",
    "#dfa_xtrain_emg = np.reshape(dfa_xtrain_emg.values, (-1,))\n",
    "\n",
    "dfa_xtest_eeg1 = pd.read_csv('dfa_xtest_eeg1.csv', header = None)\n",
    "#dfa_xtest_eeg1 = np.reshape(dfa_xtest_eeg1.values, (-1,))\n",
    "\n",
    "dfa_xtest_eeg2 = pd.read_csv('dfa_xtest_eeg2.csv', header = None)\n",
    "#dfa_xtest_eeg2 = np.reshape(dfa_xtest_eeg2.values, (-1,))\n",
    "\n",
    "dfa_xtest_emg = pd.read_csv('dfa_xtest_emg.csv', header = None)\n",
    "#dfa_xtest_emg = np.reshape(dfa_xtest_emg.values, (-1,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''update xtrain, xtest'''\n",
    "\n",
    "xtrain_comp = np.concatenate((xtrain_filter, dfa_xtrain_eeg1, dfa_xtrain_eeg2, dfa_xtrain_emg), axis = 1)\n",
    "xtest_comp = np.concatenate((xtest_filter, dfa_xtest_eeg1, dfa_xtest_eeg2, dfa_xtest_emg), axis = 1)\n",
    "\n",
    "'''standardise'''\n",
    "\n",
    "xtrain_rescaled = preprocessing.StandardScaler().fit_transform(xtrain_comp)\n",
    "xtest_rescaled = preprocessing.StandardScaler().fit_transform(xtest_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43200, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa_xtest_eeg1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ypred_svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d78e18238635>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mypred_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ypred_svm' is not defined"
     ]
    }
   ],
   "source": [
    "ypred_svm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM done\n",
      "Gradient Boosting done\n",
      "Multinomial Logistic done\n",
      "Random Forest Done\n",
      "KNN done\n",
      "SGD done\n",
      "CRF Done\n",
      "RNN Done\n"
     ]
    }
   ],
   "source": [
    "'''SVM'''\n",
    "clf_svm = svm.SVC(kernel='rbf', C = 0.01, gamma = 'auto', class_weight = 'balanced')\n",
    "clf_svm.fit(xtrain_rescaled, ytrain)\n",
    "ypred_svm = clf_svm.predict(xtest_rescaled)\n",
    "print('SVM done')\n",
    "\n",
    "\n",
    "'''Gradient Boosting'''\n",
    "clf_boost = GradientBoostingClassifier(n_estimators = 200, max_depth = 8, learning_rate = 0.1, max_features = 20)\n",
    "clf_boost.fit(xtrain_rescaled, ytrain)\n",
    "ypred_boost = clf_boost.predict(xtest_rescaled)\n",
    "print('Gradient Boosting done')\n",
    "\n",
    "\n",
    "'''Multinomial Logistic'''\n",
    "clf_logi = LogisticRegression(random_state=10, C = 0.01, class_weight = 'balanced', max_iter = 10, \n",
    "                              multi_class='auto', penalty = 'l2', solver = 'liblinear')\n",
    "clf_logi.fit(xtrain_rescaled, ytrain)\n",
    "ypred_logi = clf_logi.predict(xtest_rescaled)\n",
    "print('Multinomial Logistic done')\n",
    "\n",
    "'''Random Forest'''\n",
    "clf_RandFor = RandomForestClassifier(max_depth=5, random_state=1, class_weight = 'balanced', \n",
    "                         min_samples_leaf = 100, min_samples_split = 100, n_estimators = 20)\n",
    "clf_RandFor.fit(xtrain_rescaled, ytrain)\n",
    "ypred_RandFor = clf_RandFor.predict(xtest_rescaled)\n",
    "print('Random Forest Done')\n",
    "\n",
    "\n",
    "'''KNN'''\n",
    "clf_KNN = KNeighborsClassifier(leaf_size = 200, n_neighbors = 20, weights = 'uniform')\n",
    "clf_KNN.fit(xtrain_rescaled, ytrain)\n",
    "ypred_KNN = clf_KNN.predict(xtest_rescaled)\n",
    "print('KNN done')\n",
    "\n",
    "\n",
    "'''SGD'''\n",
    "clf_SGD = SGDClassifier(alpha = 0.0001, learning_rate = 'optimal', loss = 'hinge',\n",
    "                       max_iter = 100, penalty = 'l2', power_t = 2, shuffle = True)\n",
    "clf_SGD.fit(xtrain_rescaled, ytrain)\n",
    "ypred_SGD = clf_SGD.predict(xtest_rescaled)\n",
    "print('SGD done')\n",
    "\n",
    "\n",
    "'''CRF'''\n",
    "ypred_crf = pd.read_csv(\"ypred_crf.csv\").drop(\"Id\", axis = 1)\n",
    "ypred_crf = np.reshape(ypred_crf.values, (-1,))\n",
    "print('CRF Done')\n",
    "\n",
    "'''RNN'''\n",
    "ypred_RNN = pd.read_csv(\"RNN_13_32_0.1_0.2_a_32.csv\").drop(\"Id\", axis = 1)\n",
    "ypred_RNN = np.reshape(ypred_RNN.values, (-1,))\n",
    "print('RNN Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Classifier -- Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = hard_voter(ypred_crf, ypred_svm, ypred_logi, ypred_KNN, ypred_RNN, ypred_SGD)\n",
    "index = pd.read_csv(\"sample.csv\")\n",
    "index['y'] = submission\n",
    "index.to_csv(\"hard_voter.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
